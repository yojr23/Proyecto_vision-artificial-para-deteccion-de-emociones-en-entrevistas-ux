# AGRIOT UX - DocumentaciÃ³n TÃ©cnica Completa

## ğŸ“‹ Tabla de Contenidos

1. [DescripciÃ³n General del Sistema](#descripciÃ³n-general)
2. [Requerimientos Funcionales](#requerimientos-funcionales)
3. [Stack TecnolÃ³gico](#stack-tecnolÃ³gico)
4. [Manual de Usuario](#manual-de-usuario)
5. [Arquitectura y Estructura de Carpetas](#arquitectura-y-estructura)
6. [Relaciones entre MÃ³dulos](#relaciones-entre-mÃ³dulos)
7. [Principios SOLID y POO](#principios-solid-y-poo)
8. [DocumentaciÃ³n TÃ©cnica Detallada](#documentaciÃ³n-tÃ©cnica)

---

## ğŸ¯ DescripciÃ³n General

**AGRIOT (Semillero de InnovaciÃ³n AgrÃ­cola UX)** es una plataforma de investigaciÃ³n UX especializada en contextos agrÃ­colas rurales que integra:

- **Captura audiovisual** de entrevistas con control temporal de preguntas
- **AnotaciÃ³n temporal** mediante marcas de inicio/fin por pregunta
- **SegmentaciÃ³n automÃ¡tica** de fragmentos de video basada en marcas temporales
- **AnÃ¡lisis emocional** mediante redes neuronales convolucionales (CNN) con TensorFlow
- **DetecciÃ³n facial** utilizando MediaPipe para extracciÃ³n de regiones de interÃ©s
- **GeneraciÃ³n de reportes** accionables con mÃ©tricas de intensidad emocional
- **Interfaz grÃ¡fica** desarrollada con PySide6 (Qt) y componentes web embebidos

El sistema estÃ¡ diseÃ±ado para empoderar a investigadores UX en la evaluaciÃ³n de experiencias de usuario en entornos rurales, con especial enfoque en mujeres campesinas y comunidades agrÃ­colas.

---

## ğŸ“ Requerimientos Funcionales

### RF-01: GestiÃ³n de Entrevistas Audiovisuales
**DescripciÃ³n:** El sistema debe permitir registrar entrevistas completas con captura simultÃ¡nea de video y audio, controlando el flujo de preguntas mediante marcas temporales.

**Especificaciones:**
- Captura de video en resoluciÃ³n 720p (1280x720) con codec H.264
- Captura de audio en formato AAC
- GeneraciÃ³n automÃ¡tica de IDs Ãºnicos para entrevistas (`YYYY-MM-DD_NNN`)
- Persistencia de videos originales en `data/videos_originales/`
- Control de estado de grabaciÃ³n (iniciar/detener)
- Vista previa en tiempo real de la captura

**ImplementaciÃ³n:** `classes/entrevista.py`, `ui/interview_screen.py`, `video_io/video.py`

### RF-02: Sistema de Marcas Temporales
**DescripciÃ³n:** El sistema debe permitir marcar el inicio y fin de cada pregunta durante la entrevista, asociando notas opcionales.

**Especificaciones:**
- Marca de inicio de pregunta con timestamp relativo
- Marca de fin de pregunta con validaciÃ³n de secuencia
- AsociaciÃ³n de notas textuales por pregunta
- ValidaciÃ³n de solapamientos temporales
- Persistencia inmediata en JSON (`data/marcas/marcas_<id>.json`)
- IdentificaciÃ³n Ãºnica de preguntas mediante `pregunta_id`

**ImplementaciÃ³n:** `classes/marca.py`, `classes/marcas.py`

### RF-03: Prototipos Visuales Sincronizados
**DescripciÃ³n:** El sistema debe presentar al entrevistado prototipos visuales (semÃ¡foro, pictogramas, tablas) sincronizados con variables interactivas controladas por el entrevistador.

**Especificaciones:**
- Panel de control web embebido en QtWebEngine (`ui/entrevista_ui/controls.html`)
- Pantalla de entrevistado independiente (`ui/interviewee_screen.py`)
- SincronizaciÃ³n bidireccional mediante QWebChannel y DataBridge
- ActualizaciÃ³n en tiempo real de variables climÃ¡ticas y estados
- VisualizaciÃ³n de preguntas actuales en ambas pantallas

**ImplementaciÃ³n:** `ui/interview_screen.py`, `ui/interviewee_screen.py`, `ui/entrevista_ui/*.html`

### RF-04: GestiÃ³n de Cuestionarios
**DescripciÃ³n:** El sistema debe permitir versionar y editar cuestionarios por categorÃ­as para diferentes estudios UX.

**Especificaciones:**
- CRUD completo de preguntas por categorÃ­a
- CategorÃ­as predefinidas: General, SemÃ¡foro, Pictogramas, Tabla, ComparaciÃ³n, Cierre
- ImportaciÃ³n/exportaciÃ³n de cuestionarios en JSON
- Reordenamiento lÃ³gico de preguntas
- ValidaciÃ³n de estructura de datos

**ImplementaciÃ³n:** `ui/config_screen.py`, `classes/entrevista_preguntas.py`

### RF-05: FragmentaciÃ³n AutomÃ¡tica de Videos
**DescripciÃ³n:** El sistema debe cortar automÃ¡ticamente cada pregunta en fragmentos MP4 individuales basados en las marcas temporales.

**Especificaciones:**
- Corte frame-preciso utilizando FFmpeg con `-ss` y `-t`
- ReencodificaciÃ³n con H.264/AAC para compatibilidad
- GeneraciÃ³n de nombres de archivo: `fragmento_<entrevista_id>_<pregunta_id>.mp4`
- ValidaciÃ³n de existencia de video original
- Manejo de errores con logging detallado

**ImplementaciÃ³n:** `classes/fragmento.py`

### RF-06: AnÃ¡lisis Emocional con IA
**DescripciÃ³n:** El sistema debe analizar fragmentos de video utilizando modelos CNN de detecciÃ³n de emociones faciales, almacenando intensidades promedio y emociÃ³n dominante.

**Especificaciones:**
- Carga de modelos TensorFlow/Keras (`.h5` o `.keras`)
- DetecciÃ³n facial con MediaPipe FaceDetection
- Preprocesamiento de frames: BGRâ†’RGB, resize 224x224, normalizaciÃ³n [0,1]
- PredicciÃ³n de 7 emociones: angry, contempt, disgust, fear, happy, sad, surprise
- CÃ¡lculo de intensidades promedio por emociÃ³n
- IdentificaciÃ³n de emociÃ³n dominante con nivel de confianza
- Procesamiento asÃ­ncrono en hilo separado para no bloquear UI

**ImplementaciÃ³n:** `classes/analisis.py`, `ui/analisis_screens/analisis_generar_screen.py`

### RF-07: GeneraciÃ³n de Reportes
**DescripciÃ³n:** El sistema debe centralizar resultados en reportes interactivos con mÃ©tricas globales y capacidades de exportaciÃ³n.

**Especificaciones:**
- Dashboard de resumen con promedios por emociÃ³n
- VisualizaciÃ³n de resultados detallados por fragmento
- ExportaciÃ³n a PDF/CSV (mediante fpdf2)
- AgregaciÃ³n de mÃ©tricas por entrevista
- VisualizaciÃ³n de notas asociadas a preguntas

**ImplementaciÃ³n:** `ui/reportes_screen.py`, `classes/reporte_entrevista.py`

### RF-08: GestiÃ³n de Directorios y Logging
**DescripciÃ³n:** El sistema debe gestionar directorios de datos, logs y resultados de forma auto-contenida con trazabilidad completa.

**Especificaciones:**
- CreaciÃ³n automÃ¡tica de directorios base al iniciar
- Logging rotado por ejecuciÃ³n (`logs/debug_YYYYMMDD_HHMMSS.log`)
- Encoding UTF-8 para logs
- Niveles de logging configurables (INFO/DEBUG)
- Trazabilidad de errores en FFmpeg, carga de modelos y procesos largos

**ImplementaciÃ³n:** `main.py`

---

## ğŸ› ï¸ Stack TecnolÃ³gico

### Lenguaje y VersiÃ³n
- **Python 3.8.10** (requerido por compatibilidad con TensorFlow 2.x)

### Framework de Interfaz GrÃ¡fica
- **PySide6 (Qt 6.2+)** - Framework principal para GUI de escritorio
  - `QtWidgets` - Componentes de interfaz (ventanas, botones, layouts)
  - `QtWebEngine` - Motor web para embebido de HTML/CSS/JS
  - `QWebChannel` - ComunicaciÃ³n bidireccional Pythonâ†”JavaScript
  - `QThread` - Procesamiento asÃ­ncrono para anÃ¡lisis

### VisiÃ³n por Computador
- **OpenCV 4.5+** (`opencv-python`)
  - Captura de video en tiempo real (`cv2.VideoCapture`)
  - Procesamiento de frames (conversiÃ³n de color, resize)
  - VideoWriter para grabaciÃ³n (fallback Linux)

- **MediaPipe** (dependencia implÃ­cita)
  - `FaceDetection` - DetecciÃ³n de rostros con bounding boxes
  - Modelo de detecciÃ³n: `model_selection=1` (rango completo)
  - Confianza mÃ­nima: `min_detection_confidence=0.5`

### Machine Learning
- **TensorFlow 2.6+** / **Keras**
  - Carga de modelos CNN pre-entrenados (`.h5`, `.keras`)
  - Inferencia sin compilaciÃ³n (`compile=False`)
  - Compatibilidad con modelos antiguos mediante parches de `InputLayer`

### Procesamiento Multimedia
- **FFmpeg** (binario externo)
  - Captura en macOS: `avfoundation` (cÃ¡mara + micrÃ³fono)
  - Corte de fragmentos: `-ss` (seek) + `-t` (duraciÃ³n)
  - ReencodificaciÃ³n: `libx264` (video), `aac` (audio)
  - Preset: `ultrafast` para procesamiento rÃ¡pido

- **PyAudio 0.2.11+**
  - Captura de audio en Windows
  - Formato: `paInt16`, 44100 Hz, mono

### Procesamiento NumÃ©rico
- **NumPy 1.19-1.26**
  - Operaciones con tensores (arrays multidimensionales)
  - NormalizaciÃ³n de imÃ¡genes
  - ExpansiÃ³n de dimensiones para batch processing

### VisualizaciÃ³n y Reportes
- **Matplotlib 3.3+** - GrÃ¡ficos y visualizaciones (opcional)
- **fpdf2 2.4+** - GeneraciÃ³n de PDFs para exportaciÃ³n

### Utilidades
- **Pathlib** - Manejo de rutas multiplataforma
- **Dataclasses** - Estructuras de datos inmutables (`@dataclass`)
- **Logging** - Sistema de logging estÃ¡ndar de Python
- **Subprocess** - EjecuciÃ³n de procesos externos (FFmpeg)

### Arquitectura de Dependencias

```mermaid
graph TB
    A[Python 3.8.10] --> B[PySide6]
    A --> C[OpenCV]
    A --> D[TensorFlow/Keras]
    A --> E[NumPy]
    
    B --> F[QtWidgets]
    B --> G[QtWebEngine]
    B --> H[QWebChannel]
    
    C --> I[MediaPipe]
    D --> J[Modelos CNN .h5/.keras]
    
    K[FFmpeg] -.->|Proceso externo| L[Captura/GrabaciÃ³n]
    M[PyAudio] -.->|Solo Windows| L
    
    style A fill:#3776ab
    style B fill:#41cd52
    style D fill:#ff6f00
    style C fill:#5c3ee8
```

---

## ğŸ‘¤ Manual de Usuario

### InstalaciÃ³n y ConfiguraciÃ³n Inicial

#### Prerrequisitos
1. **Python 3.8.10** (recomendado usar `pyenv`):
   ```bash
   pyenv install 3.8.10
   pyenv local 3.8.10
   ```

2. **FFmpeg** instalado y disponible en PATH:
   ```bash
   # macOS
   brew install ffmpeg
   
   # Verificar instalaciÃ³n
   ffmpeg -version
   ```

3. **Permisos de sistema:**
   - macOS: Permisos de cÃ¡mara y micrÃ³fono en Preferencias del Sistema
   - Windows: Permisos de cÃ¡mara y micrÃ³fono en ConfiguraciÃ³n

#### InstalaciÃ³n de Dependencias
```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # macOS/Linux
# o
venv\Scripts\activate  # Windows

# Instalar dependencias base
pip install -r requirements.txt

# Instalar TensorFlow (opcional, solo si se usarÃ¡ anÃ¡lisis)
pip install tensorflow>=2.6,<3.0

# Instalar MediaPipe (requerido para anÃ¡lisis)
pip install mediapipe
```

#### EjecuciÃ³n
```bash
# Modo normal
python main.py

# Modo debug (logs verbosos)
python main.py --debug
```

### Flujo de Trabajo Completo

#### 1. ConfiguraciÃ³n del Cuestionario

**Objetivo:** Preparar las preguntas que se realizarÃ¡n durante la entrevista.

1. Desde la pantalla principal, hacer clic en **"âš¡ Configurar Sistema"**
2. En la secciÃ³n **"GestiÃ³n de Preguntas"**:
   - Seleccionar una categorÃ­a (General, SemÃ¡foro, Pictogramas, etc.)
   - Hacer clic en **"â• Agregar Pregunta"**
   - Completar el texto de la pregunta
   - Guardar
3. Reordenar preguntas arrastrando (si estÃ¡ disponible)
4. Exportar cuestionario: **"ğŸ’¾ Exportar preguntas.json"** (opcional)

**Resultado:** Las preguntas se guardan en `data/preguntas.json` y estarÃ¡n disponibles para todas las entrevistas futuras.

#### 2. RealizaciÃ³n de Entrevista

**Objetivo:** Capturar video/audio de la entrevista con marcas temporales por pregunta.

1. Desde la pantalla principal, hacer clic en **"ğŸ“ Iniciar Entrevista"**
2. **PreparaciÃ³n:**
   - Verificar que la cÃ¡mara y micrÃ³fono estÃ©n funcionando (vista previa visible)
   - Ajustar resoluciÃ³n si es necesario (por defecto 720p)
3. **Iniciar grabaciÃ³n:**
   - Hacer clic en **"ğŸ¥ Iniciar GrabaciÃ³n"**
   - El sistema propone un ID: `entrevista_YYYY-MM-DD_NNN.mp4`
   - Confirmar o modificar el ID
4. **Durante la entrevista:**
   - Para cada pregunta:
     - Hacer clic en **"â–¶ï¸ Marcar Inicio Pregunta"** cuando se formula la pregunta
     - El sistema avanza automÃ¡ticamente a la siguiente pregunta en el cuestionario
     - AÃ±adir notas en el campo de texto durante la respuesta del entrevistado
     - Hacer clic en **"â¹ï¸ Marcar Fin Pregunta"** cuando termina la respuesta
   - Controlar variables en el panel web (temperatura, humedad, etc.) - se sincronizan con la pantalla del entrevistado
5. **Finalizar:**
   - Hacer clic en **"â¹ï¸ Detener GrabaciÃ³n"**
   - El sistema genera automÃ¡ticamente:
     - Video completo en `data/videos_originales/entrevista_<id>.mp4`
     - Archivo de marcas en `data/marcas/marcas_<id>.json`

**Pantalla del Entrevistado:**
- Abrir la pantalla del entrevistado en un segundo monitor/dispositivo
- Esta pantalla muestra los prototipos visuales sincronizados
- Las variables controladas por el entrevistador se actualizan en tiempo real

#### 3. GeneraciÃ³n de Fragmentos

**Objetivo:** Cortar el video completo en fragmentos individuales por pregunta.

1. Desde la pantalla principal, hacer clic en **"ğŸ¥ PrevisualizaciÃ³n de Fragmentos"**
2. **SelecciÃ³n de entrevista:**
   - En la pestaÃ±a **"InformaciÃ³n"**, seleccionar una entrevista de la lista
   - Verificar que aparezcan las marcas asociadas
3. **GeneraciÃ³n:**
   - Ir a la pestaÃ±a **"Generar Fragmentos"**
   - Seleccionar el video original de la entrevista
   - Hacer clic en **"âœ‚ï¸ Generar Fragmentos"**
   - El sistema procesa cada marca y genera archivos MP4 individuales
4. **VerificaciÃ³n:**
   - En la pestaÃ±a **"Fragmentos"**, ver la lista de fragmentos generados
   - Reproducir fragmentos para verificar calidad

**Resultado:** Fragmentos guardados en `data/fragmentos/fragmento_<entrevista_id>_<pregunta_id>.mp4`

#### 4. AnÃ¡lisis Emocional

**Objetivo:** Analizar fragmentos con modelos de IA para detectar emociones.

1. Desde la pantalla principal, hacer clic en **"ğŸ“ˆ Generar AnÃ¡lisis"**
2. **SelecciÃ³n:**
   - Seleccionar una entrevista de la lista
   - Seleccionar los fragmentos a analizar (checkbox)
   - Seleccionar el modelo CNN (`ml/cp_best_finetuned.h5` o similar)
3. **Procesamiento:**
   - Hacer clic en **"ğŸš€ Iniciar AnÃ¡lisis"**
   - El sistema procesa cada fragmento en un hilo separado (UI no se congela)
   - Ver progreso en la barra y logs en tiempo real
4. **Resultados:**
   - Los resultados se guardan automÃ¡ticamente en `data/resultados/<entrevista_id>/`
   - Cada fragmento genera un JSON con:
     - Intensidades promedio por emociÃ³n
     - EmociÃ³n dominante
     - Confianza
     - Metadatos (fecha, modelo usado, frames analizados)

**Nota:** El anÃ¡lisis puede tardar varios minutos dependiendo del nÃºmero de fragmentos y la longitud de los videos.

#### 5. VisualizaciÃ³n de Reportes

**Objetivo:** Revisar resultados agregados y exportar reportes.

1. Desde la pantalla principal, hacer clic en **"ğŸ“Š Ver Resultados"**
2. **Resumen:**
   - Seleccionar una entrevista
   - Ver dashboard con:
     - Promedios de intensidad por emociÃ³n (grÃ¡fico de barras)
     - EmociÃ³n dominante general
     - NÃºmero de preguntas analizadas
     - DuraciÃ³n total
3. **Detalles:**
   - Ver tabla con resultados por fragmento/pregunta
   - Filtrar por emociÃ³n dominante
   - Ver notas asociadas
4. **ExportaciÃ³n:**
   - Hacer clic en **"ğŸ“„ Exportar PDF"** o **"ğŸ“Š Exportar CSV"**
   - Seleccionar ubicaciÃ³n de guardado

---

## ğŸ—ï¸ Arquitectura y Estructura de Carpetas

### Estructura de Directorios

```
proyecto_ux/
â”œâ”€â”€ main.py                          # Punto de entrada principal
â”œâ”€â”€ requirements.txt                 # Dependencias del proyecto
â”‚
â”œâ”€â”€ ui/                              # Capa de PresentaciÃ³n (PySide6)
â”‚   â”œâ”€â”€ app.py                       # AplicaciÃ³n principal (QMainWindow)
â”‚   â”œâ”€â”€ interview_screen.py          # Pantalla de entrevista (entrevistador)
â”‚   â”œâ”€â”€ interviewee_screen.py        # Pantalla de entrevistado
â”‚   â”œâ”€â”€ fragmento_screen.py          # Ventana principal de fragmentos
â”‚   â”œâ”€â”€ analisis_screen.py           # Ventana principal de anÃ¡lisis
â”‚   â”œâ”€â”€ reportes_screen.py           # Ventana principal de reportes
â”‚   â”œâ”€â”€ config_screen.py             # ConfiguraciÃ³n del sistema
â”‚   â”‚
â”‚   â”œâ”€â”€ entrevista_ui/               # Prototipos web embebidos
â”‚   â”‚   â”œâ”€â”€ controls.html            # Panel de control (entrevistador)
â”‚   â”‚   â”œâ”€â”€ index.html               # Vista principal entrevistado
â”‚   â”‚   â””â”€â”€ tarjetas.html            # Componentes visuales
â”‚   â”‚
â”‚   â”œâ”€â”€ fragmento_screens/           # Sub-pantallas de fragmentos
â”‚   â”‚   â”œâ”€â”€ fragmento_info_screen.py      # InformaciÃ³n de entrevistas
â”‚   â”‚   â”œâ”€â”€ fragmento_generar_screen.py   # GeneraciÃ³n de fragmentos
â”‚   â”‚   â””â”€â”€ fragmento_fragmentos_screen.py # Lista de fragmentos
â”‚   â”‚
â”‚   â”œâ”€â”€ analisis_screens/            # Sub-pantallas de anÃ¡lisis
â”‚   â”‚   â”œâ”€â”€ analisis_info_screen.py       # SelecciÃ³n de entrevista
â”‚   â”‚   â”œâ”€â”€ analisis_generar_screen.py    # GeneraciÃ³n de anÃ¡lisis
â”‚   â”‚   â””â”€â”€ analisis_reporte_screen.py    # VisualizaciÃ³n de resultados
â”‚   â”‚
â”‚   â”œâ”€â”€ reportes_screens/            # Sub-pantallas de reportes
â”‚   â”‚   â”œâ”€â”€ resumen_screen.py        # Dashboard de resumen
â”‚   â”‚   â”œâ”€â”€ detalle_screen.py        # Tabla detallada
â”‚   â”‚   â””â”€â”€ export_screen.py         # ExportaciÃ³n PDF/CSV
â”‚   â”‚
â”‚   â”œâ”€â”€ config_screens/              # Sub-pantallas de configuraciÃ³n
â”‚   â”‚   â””â”€â”€ config_pregunta_screen.py # GestiÃ³n de preguntas
â”‚   â”‚
â”‚   â”œâ”€â”€ informacion_adicional/       # Pantallas informativas
â”‚   â”‚   â”œâ”€â”€ deteccion_screen.py      # Info sobre detecciÃ³n emocional
â”‚   â”‚   â”œâ”€â”€ ux_agricola_screen.py    # Info sobre UX agrÃ­cola
â”‚   â”‚   â””â”€â”€ transformacion_screen.py # Info sobre transformaciÃ³n digital
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # Componentes reutilizables
â”‚       â”œâ”€â”€ buttons.py               # Botones modernos estilizados
â”‚       â”œâ”€â”€ cards.py                 # Cards flotantes y animadas
â”‚       â”œâ”€â”€ animations.py            # Animaciones de labels y tÃ­tulos
â”‚       â”œâ”€â”€ styles.py                # Paletas de colores y estilos
â”‚       â””â”€â”€ footer.py                # Footer animado
â”‚
â”œâ”€â”€ classes/                         # Capa de Dominio (LÃ³gica de Negocio)
â”‚   â”œâ”€â”€ entrevista.py                # Entidad principal: Entrevista
â”‚   â”œâ”€â”€ marca.py                     # Entidad: Marca temporal (@dataclass)
â”‚   â”œâ”€â”€ marcas.py                    # Agregado: ColecciÃ³n de marcas
â”‚   â”œâ”€â”€ fragmento.py                 # Entidad: Fragmento de video
â”‚   â”œâ”€â”€ analisis.py                  # Servicio: AnÃ¡lisis emocional
â”‚   â”œâ”€â”€ reporte_entrevista.py        # Entidad: Reporte de entrevista
â”‚   â”œâ”€â”€ entrevista_preguntas.py      # Gestor de cuestionarios
â”‚   â””â”€â”€ reporte.py                   # Utilidades de reportes
â”‚
â”œâ”€â”€ video_io/                        # Capa de Infraestructura (I/O)
â”‚   â””â”€â”€ video.py                     # AbstracciÃ³n de captura multiplataforma
â”‚       â”œâ”€â”€ CapturadorVideo (ABC)    # Interfaz abstracta
â”‚       â”œâ”€â”€ CapturadorVideoMacOS     # ImplementaciÃ³n macOS (FFmpeg)
â”‚       â”œâ”€â”€ CapturadorVideoWindows   # ImplementaciÃ³n Windows (OpenCV+PyAudio)
â”‚       â””â”€â”€ CapturadorVideoLinux     # ImplementaciÃ³n Linux (OpenCV)
â”‚
â”œâ”€â”€ ml/                              # Modelos de Machine Learning
â”‚   â”œâ”€â”€ cp_best_finetuned.h5         # Modelo CNN (formato HDF5)
â”‚   â”œâ”€â”€ cp_best_finetuned.keras      # Modelo CNN (formato Keras)
â”‚   â””â”€â”€ script.py                    # Scripts de conversiÃ³n de modelos
â”‚
â”œâ”€â”€ data/                            # Persistencia de Datos
â”‚   â”œâ”€â”€ videos_originales/           # Videos completos de entrevistas
â”‚   â”œâ”€â”€ fragmentos/                  # Fragmentos MP4 por pregunta
â”‚   â”œâ”€â”€ marcas/                      # Archivos JSON de marcas temporales
â”‚   â”œâ”€â”€ resultados/                  # Resultados de anÃ¡lisis emocional
â”‚   â”‚   â””â”€â”€ <entrevista_id>/         # Por entrevista
â”‚   â”‚       â””â”€â”€ resultados_fragmento_<pregunta>.json
â”‚   â”œâ”€â”€ reportes/                    # Reportes exportados (PDF/CSV)
â”‚   â”œâ”€â”€ entrevistas/                 # Metadatos de entrevistas
â”‚   â”œâ”€â”€ preguntas.json               # Cuestionarios guardados
â”‚   â””â”€â”€ dataset/                     # Datasets de entrenamiento (opcional)
â”‚
â”œâ”€â”€ logs/                            # Logs de EjecuciÃ³n
â”‚   â””â”€â”€ debug_YYYYMMDD_HHMMSS.log    # Logs rotados por ejecuciÃ³n
â”‚
â””â”€â”€ img/                             # Recursos de Imagen
    â”œâ”€â”€ semillin.png                 # Mascota del sistema
    â””â”€â”€ mascota_agriot.png           # Logo alternativo
```

### Diagrama de Arquitectura por Capas

```mermaid
graph TB
    subgraph "Capa de PresentaciÃ³n (UI)"
        A1[app.py - QMainWindow]
        A2[interview_screen.py]
        A3[interviewee_screen.py]
        A4[fragmento_screen.py]
        A5[analisis_screen.py]
        A6[reportes_screen.py]
        A7[QtWebEngine + HTML]
    end
    
    subgraph "Capa de Dominio (Business Logic)"
        B1[Entrevista]
        B2[Marca / Marcas]
        B3[Fragmento]
        B4[Analisis]
        B5[ReporteEntrevista]
    end
    
    subgraph "Capa de Infraestructura (I/O)"
        C1[video_io/video.py]
        C2[FFmpeg Process]
        C3[OpenCV VideoCapture]
        C4[FileSystem JSON]
        C5[TensorFlow Model]
        C6[MediaPipe]
    end
    
    A1 --> A2
    A1 --> A4
    A1 --> A5
    A1 --> A6
    A2 --> A7
    A3 --> A7
    
    A2 --> B1
    A4 --> B3
    A5 --> B4
    A6 --> B5
    
    B1 --> B2
    B1 --> B3
    B3 --> B4
    B4 --> B5
    
    B1 --> C1
    B3 --> C2
    B4 --> C5
    B4 --> C6
    B5 --> C4
    
    C1 --> C2
    C1 --> C3
    
    style A1 fill:#e1f5ff
    style B1 fill:#fff4e1
    style C1 fill:#e8f5e9
```

---

## ğŸ”— Relaciones entre MÃ³dulos

### Diagrama de Dependencias

```mermaid
graph LR
    subgraph "MÃ³dulo Principal"
        MAIN[main.py]
    end
    
    subgraph "MÃ³dulo UI"
        APP[app.py]
        INT[interview_screen]
        FRAG[fragmento_screen]
        ANAL[analisis_screen]
        REP[reportes_screen]
    end
    
    subgraph "MÃ³dulo Dominio"
        ENT[Entrevista]
        MAR[Marca/Marcas]
        FRAG_CLS[Fragmento]
        ANA_CLS[Analisis]
        REP_CLS[ReporteEntrevista]
    end
    
    subgraph "MÃ³dulo Infraestructura"
        VID[video_io]
        FF[FFmpeg]
        CV[OpenCV]
        TF[TensorFlow]
        MP[MediaPipe]
    end
    
    MAIN --> APP
    APP --> INT
    APP --> FRAG
    APP --> ANAL
    APP --> REP
    
    INT --> ENT
    FRAG --> FRAG_CLS
    ANAL --> ANA_CLS
    REP --> REP_CLS
    
    ENT --> MAR
    ENT --> VID
    FRAG_CLS --> FF
    ANA_CLS --> TF
    ANA_CLS --> MP
    ANA_CLS --> CV
    
    VID --> FF
    VID --> CV
    
    style MAIN fill:#ff6b6b
    style APP fill:#4ecdc4
    style ENT fill:#95e1d3
    style VID fill:#f38181
```

### Flujo de Datos Completo

```mermaid
sequenceDiagram
    participant U as Usuario
    participant UI as InterviewScreen
    participant E as Entrevista
    participant M as Marcas
    participant V as VideoIO
    participant F as Fragmento
    participant A as Analisis
    participant R as ReporteEntrevista
    
    U->>UI: Iniciar GrabaciÃ³n
    UI->>E: iniciar()
    E->>V: iniciar_grabacion()
    V->>FFmpeg: Proceso de captura
    
    U->>UI: Marcar Inicio Pregunta
    UI->>E: marcar_inicio_pregunta()
    E->>M: agregar_marca()
    M->>JSON: exportar_json()
    
    U->>UI: Marcar Fin Pregunta
    UI->>E: marcar_fin_pregunta()
    E->>M: actualizar_marca()
    
    U->>UI: Detener GrabaciÃ³n
    UI->>E: finalizar()
    E->>V: detener_grabacion()
    E->>F: generar_fragmento() (por cada marca)
    F->>FFmpeg: Corte de video
    E->>R: agregar_pregunta()
    
    U->>UI: Generar AnÃ¡lisis
    UI->>A: analizar_fragmento()
    A->>MediaPipe: DetecciÃ³n facial
    A->>TensorFlow: PredicciÃ³n emocional
    A->>JSON: Guardar resultados
    
    U->>UI: Ver Reportes
    UI->>R: generar_resumen()
    R->>JSON: Cargar resultados
    R-->>UI: MÃ©tricas agregadas
```

### Relaciones de ComposiciÃ³n y AgregaciÃ³n

```mermaid
classDiagram
    class Entrevista {
        -id: str
        -marcas: Marcas
        -fragmentos: List[Fragmento]
        -reporte: ReporteEntrevista
        +iniciar()
        +finalizar()
        +marcar_inicio_pregunta()
        +marcar_fin_pregunta()
    }
    
    class Marcas {
        -entrevista_id: str
        -marcas: List[Marca]
        +agregar_marca()
        +buscar_marcas_por_pregunta_id()
        +exportar_json()
    }
    
    class Marca {
        +entrevista_id: str
        +pregunta_id: int
        +inicio: float
        +fin: Optional[float]
        +nota: str
    }
    
    class Fragmento {
        -marca: Marca
        -ruta_fragmento: Path
        +generar_fragmento()
    }
    
    class Analisis {
        -model: TensorFlow Model
        -face_detection: MediaPipe
        +analizar_fragmento()
        +get_emotion_summary()
    }
    
    class ReporteEntrevista {
        -id: str
        -preguntas: List[Dict]
        +generar_resumen()
        +exportar_json()
    }
    
    Entrevista *-- Marcas : composiciÃ³n
    Entrevista *-- ReporteEntrevista : composiciÃ³n
    Entrevista o-- Fragmento : agregaciÃ³n
    Marcas *-- Marca : agregaciÃ³n
    Fragmento --> Marca : usa
    Analisis --> Fragmento : analiza
    ReporteEntrevista --> Analisis : consume resultados
```

---

## ğŸ¯ Principios SOLID y POO

### Single Responsibility Principle (SRP)

**Cumplimiento: 9/10**

Cada clase tiene una responsabilidad Ãºnica y bien definida:

#### Ejemplos de Cumplimiento:

**`Marca` (dataclass):**
- **Responsabilidad Ãºnica:** Representar una marca temporal con validaciÃ³n de datos
- No maneja persistencia, no maneja lÃ³gica de negocio compleja

```python
@dataclass
class Marca:
    entrevista_id: str
    pregunta_id: int
    inicio: float
    fin: Optional[float] = None
    nota: str = ""
    # Solo validaciÃ³n y cÃ¡lculo de duraciÃ³n
```

**`Fragmento`:**
- **Responsabilidad Ãºnica:** Generar fragmentos de video mediante FFmpeg
- No maneja anÃ¡lisis, no maneja UI

```python
class Fragmento:
    def generar_fragmento(self, video_original: Path):
        # Solo ejecuta FFmpeg para cortar video
```

**`Analisis`:**
- **Responsabilidad Ãºnica:** Analizar fragmentos de video con modelos de IA
- No maneja generaciÃ³n de fragmentos, no maneja UI

#### Ãreas de Mejora:

**`Entrevista`:**
- Actualmente maneja: grabaciÃ³n, marcas, fragmentos, reportes
- **Mejora sugerida:** Extraer lÃ³gica de grabaciÃ³n a un servicio `GrabacionService`

### Open/Closed Principle (OCP)

**Cumplimiento: 8/10**

El sistema estÃ¡ abierto para extensiÃ³n pero cerrado para modificaciÃ³n:

#### Ejemplos de Cumplimiento:

**`CapturadorVideo` (ABC):**
- Interfaz abstracta que permite agregar nuevas plataformas sin modificar cÃ³digo existente
- Nuevas implementaciones: `CapturadorVideoLinux`, `CapturadorVideoAndroid`, etc.

```python
class CapturadorVideo(ABC):
    @abstractmethod
    def iniciar_grabacion(self, ...): pass
    
class CapturadorVideoMacOS(CapturadorVideo): ...
class CapturadorVideoWindows(CapturadorVideo): ...
```

**Sistema de AnÃ¡lisis:**
- `Analisis` puede cargar diferentes modelos sin modificar la clase
- Nuevos modelos se agregan simplemente cambiando la ruta

#### Ãreas de Mejora:

**Reglas de AnÃ¡lisis:**
- Actualmente hardcodeado para 7 emociones
- **Mejora sugerida:** Strategy Pattern para diferentes tipos de anÃ¡lisis

```python
# Propuesta de mejora
class AnalisisStrategy(ABC):
    @abstractmethod
    def analizar(self, frame): pass

class EmotionAnalysisStrategy(AnalisisStrategy): ...
class GestureAnalysisStrategy(AnalisisStrategy): ...
```

### Liskov Substitution Principle (LSP)

**Cumplimiento: 10/10**

Todas las implementaciones de `CapturadorVideo` son completamente sustituibles:

```python
def obtener_capturador():
    sistema = platform.system()
    if sistema == 'Darwin':
        return CapturadorVideoMacOS()  # âœ… Sustituible
    elif sistema == 'Windows':
        return CapturadorVideoWindows()  # âœ… Sustituible
    else:
        return CapturadorVideoLinux()  # âœ… Sustituible

# Cualquier implementaciÃ³n funciona sin cambios en Entrevista
entrevista = Entrevista()
entrevista.capturador = obtener_capturador()  # Funciona con cualquiera
```

### Interface Segregation Principle (ISP)

**Cumplimiento: 7/10**

#### Ejemplos de Cumplimiento:

**`DataBridge` (QWebChannel):**
- Expone solo las seÃ±ales necesarias para comunicaciÃ³n Pythonâ†”JavaScript
- No expone mÃ©todos internos innecesarios

```python
class DataBridge(QObject):
    updateData = Signal(dict)  # Solo lo necesario
    updatePregunta = Signal(str)  # Solo lo necesario
```

#### Ãreas de Mejora:

**`Analisis`:**
- Expone tanto detecciÃ³n facial como predicciÃ³n emocional
- **Mejora sugerida:** Separar en interfaces mÃ¡s especÃ­ficas

```python
# Propuesta de mejora
class FaceDetector(ABC):
    @abstractmethod
    def detect_face(self, frame): pass

class EmotionPredictor(ABC):
    @abstractmethod
    def predict_emotion(self, face_roi): pass
```

### Dependency Inversion Principle (DIP)

**Cumplimiento: 8/10**

#### Ejemplos de Cumplimiento:

**`Entrevista` depende de abstracciÃ³n:**
```python
class Entrevista:
    def __init__(self, ...):
        # Depende de abstracciÃ³n, no de implementaciÃ³n concreta
        self.capturador = obtener_capturador()  # Retorna CapturadorVideo (ABC)
```

**InyecciÃ³n de dependencias en UI:**
```python
class AnalisisGenerarScreen:
    def __init__(self, analizador: Analisis):  # Depende de abstracciÃ³n
        self.analizador = analizador
```

#### Ãreas de Mejora:

**Dependencias directas a modelos:**
- `Analisis` carga modelos directamente desde filesystem
- **Mejora sugerida:** Inyectar `ModelLoader` como dependencia

```python
# Propuesta de mejora
class ModelLoader(ABC):
    @abstractmethod
    def load_model(self, path): pass

class Analisis:
    def __init__(self, model_loader: ModelLoader):
        self.model = model_loader.load_model(path)
```

### ProgramaciÃ³n Orientada a Objetos (POO)

#### EncapsulaciÃ³n

**Ejemplos:**
- Atributos privados con `_` (convenciÃ³n Python): `self._model`, `self._face_detection`
- Propiedades con `@property` para acceso controlado:

```python
@property
def model(self):
    return self._model  # Acceso controlado, carga perezosa
```

#### Herencia

**Ejemplos:**
- `CapturadorVideoMacOS`, `CapturadorVideoWindows` heredan de `CapturadorVideo`
- Componentes UI heredan de `QWidget`, `QMainWindow`

#### Polimorfismo

**Ejemplos:**
- Diferentes implementaciones de `CapturadorVideo` con mismo comportamiento
- `AnalysisThread` (QThread) con mÃ©todo `run()` polimÃ³rfico

#### AbstracciÃ³n

**Ejemplos:**
- `CapturadorVideo` como clase abstracta (ABC)
- MÃ©todos abstractos definen contrato sin implementaciÃ³n

#### ComposiciÃ³n y AgregaciÃ³n

**Ejemplos:**
- `Entrevista` **compone** `Marcas` y `ReporteEntrevista` (relaciÃ³n fuerte)
- `Entrevista` **agrega** `Fragmento` (relaciÃ³n dÃ©bil, lista)
- `Fragmento` **usa** `Marca` (dependencia)

#### Dataclasses

**Uso de `@dataclass` para inmutabilidad:**
```python
@dataclass
class Marca:
    entrevista_id: str
    pregunta_id: int
    inicio: float
    fin: Optional[float] = None
    nota: str = ""
```

### Diagrama de Principios SOLID Aplicados

```mermaid
graph TB
    subgraph "SRP - Responsabilidad Ãšnica"
        SRP1[Marca: Solo datos temporales]
        SRP2[Fragmento: Solo corte de video]
        SRP3[Analisis: Solo anÃ¡lisis emocional]
    end
    
    subgraph "OCP - Abierto/Cerrado"
        OCP1[CapturadorVideo ABC]
        OCP2[MacOS Implementation]
        OCP3[Windows Implementation]
        OCP4[Linux Implementation]
        OCP1 -.->|extiende| OCP2
        OCP1 -.->|extiende| OCP3
        OCP1 -.->|extiende| OCP4
    end
    
    subgraph "LSP - SustituciÃ³n de Liskov"
        LSP1[Entrevista usa CapturadorVideo]
        LSP2[Cualquier implementaciÃ³n funciona]
        LSP1 --> LSP2
    end
    
    subgraph "ISP - SegregaciÃ³n de Interfaces"
        ISP1[DataBridge: Solo seÃ±ales necesarias]
        ISP2[QWebChannel: ComunicaciÃ³n mÃ­nima]
    end
    
    subgraph "DIP - InversiÃ³n de Dependencias"
        DIP1[Entrevista depende de ABC]
        DIP2[No depende de implementaciÃ³n concreta]
        DIP1 --> DIP2
    end
    
    style SRP1 fill:#e8f5e9
    style OCP1 fill:#fff3e0
    style LSP1 fill:#e3f2fd
    style ISP1 fill:#fce4ec
    style DIP1 fill:#f3e5f5
```

---

## ğŸ“š DocumentaciÃ³n TÃ©cnica Detallada

### Arquitectura del Sistema

#### PatrÃ³n ArquitectÃ³nico: Layered Architecture (Arquitectura por Capas)

El sistema sigue una arquitectura en capas que separa responsabilidades:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Capa de PresentaciÃ³n (UI)         â”‚  PySide6, QtWebEngine
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Capa de Dominio (Business Logic)  â”‚  Entidades, Servicios
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Capa de Infraestructura (I/O)     â”‚  FFmpeg, OpenCV, TensorFlow
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Flujo de Procesamiento de AnÃ¡lisis Emocional

```mermaid
flowchart TD
    A[Fragmento MP4] --> B[OpenCV VideoCapture]
    B --> C[Frame Extraction]
    C --> D{Skip Frames?}
    D -->|SÃ­| C
    D -->|No| E[MediaPipe FaceDetection]
    E --> F{Rostro detectado?}
    F -->|No| C
    F -->|SÃ­| G[Crop Face ROI]
    G --> H[Preprocessing: BGRâ†’RGB, Resize 224x224, Normalize]
    H --> I[TensorFlow Model Predict]
    I --> J[Emotion Intensities Array]
    J --> K[Aggregate Results]
    K --> L[JSON Export]
    
    style A fill:#ffeb3b
    style I fill:#f44336
    style L fill:#4caf50
```

#### Detalles de ImplementaciÃ³n

##### 1. Captura de Video Multiplataforma

**Problema:** Diferentes sistemas operativos requieren diferentes APIs para captura de video/audio.

**SoluciÃ³n:** PatrÃ³n Strategy con clase abstracta base.

```python
# AbstracciÃ³n
class CapturadorVideo(ABC):
    @abstractmethod
    def iniciar_grabacion(self, ...): pass

# ImplementaciÃ³n macOS (FFmpeg avfoundation)
class CapturadorVideoMacOS(CapturadorVideo):
    def iniciar_grabacion(self, ruta_archivo, ...):
        subprocess.Popen([
            'ffmpeg', '-f', 'avfoundation',
            '-i', '0:1',  # cÃ¡mara:micrÃ³fono
            ...
        ])

# Factory function
def obtener_capturador():
    sistema = platform.system()
    if sistema == 'Darwin':
        return CapturadorVideoMacOS()
    # ...
```

**Ventajas:**
- Extensible a nuevas plataformas sin modificar cÃ³digo existente
- Testeable mediante mocks
- Cumple OCP y DIP

##### 2. Sistema de Marcas Temporales

**Problema:** Necesidad de marcar eventos temporales durante grabaciÃ³n con persistencia inmediata.

**SoluciÃ³n:** Agregado `Marcas` que gestiona colecciÃ³n de `Marca`.

```python
class Marcas:
    def __init__(self, entrevista_id, video_original):
        self.entrevista_id = entrevista_id
        self.marcas: List[Marca] = []
    
    def agregar_marca(self, marca: Marca):
        # ValidaciÃ³n de solapamientos
        if self._hay_solapamiento(marca):
            raise ValueError("Marca solapada")
        self.marcas.append(marca)
        self.exportar_json()  # Persistencia inmediata
```

**CaracterÃ­sticas:**
- ValidaciÃ³n de integridad (no solapamientos)
- Persistencia inmediata (resiliencia ante fallos)
- SerializaciÃ³n JSON para portabilidad

##### 3. FragmentaciÃ³n de Video con FFmpeg

**Problema:** Corte preciso de videos basado en timestamps con preservaciÃ³n de calidad.

**SoluciÃ³n:** Uso de FFmpeg con parÃ¡metros optimizados.

```python
comando = [
    'ffmpeg',
    '-ss', str(marca.inicio),    # Seek antes de -i (mÃ¡s preciso)
    '-i', str(video_original),
    '-t', str(duracion),
    '-c:v', 'libx264',           # Reencode para evitar frames negros
    '-preset', 'ultrafast',      # Balance velocidad/calidad
    '-c:a', 'aac',
    '-movflags', '+faststart',   # Streaming optimizado
    str(ruta_fragmento)
]
```

**Optimizaciones:**
- `-ss` antes de `-i`: bÃºsqueda rÃ¡pida sin decodificar todo el video
- ReencodificaciÃ³n: evita problemas de sincronizaciÃ³n
- `+faststart`: permite streaming progresivo

##### 4. AnÃ¡lisis Emocional con CNN

**Arquitectura del Modelo:**
- Input: Imagen RGB 224x224 normalizada [0,1]
- Procesamiento: Red neuronal convolucional (CNN)
- Output: Vector de 7 probabilidades (una por emociÃ³n)

**Pipeline de Procesamiento:**

```python
def analizar_fragmento(self, fragmento_path, skip_frames=1):
    cap = cv2.VideoCapture(str(fragmento_path))
    resultados = []
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # 1. DetecciÃ³n facial
        cropped = self.crop_face(frame)  # MediaPipe
        if cropped is None:
            continue
        
        # 2. Preprocesamiento
        processed = self.preprocess_frame(cropped)  # 224x224, normalizado
        
        # 3. PredicciÃ³n
        prediction = self.model.predict(processed, verbose=0)[0]
        
        # 4. Mapeo a emociones
        intensidades = {
            self.emotion_map[i]: float(prediction[i]) 
            for i in range(len(prediction))
        }
        resultados.append(intensidades)
    
    return resultados
```

**Optimizaciones:**
- `skip_frames`: Procesa cada N frames para reducir carga computacional
- Carga perezosa de MediaPipe: Solo se inicializa cuando se necesita
- Batch processing: Modelo procesa en batches para eficiencia

##### 5. SincronizaciÃ³n Bidireccional Pythonâ†”JavaScript

**Problema:** ComunicaciÃ³n en tiempo real entre aplicaciÃ³n Qt y contenido web embebido.

**SoluciÃ³n:** QWebChannel con objeto puente.

```python
# Python side
class DataBridge(QObject):
    updateData = Signal(dict)
    updatePregunta = Signal(str)
    
    @Slot(dict)
    def receiveFromJS(self, data):
        # Recibir datos de JavaScript
        pass

# JavaScript side (en HTML)
const bridge = window.bridge;
bridge.updateData.connect(function(data) {
    // Actualizar UI
});

// Enviar a Python
bridge.receiveFromJS({temperature: 25});
```

**Ventajas:**
- ComunicaciÃ³n bidireccional sin polling
- Type-safe con seÃ±ales Qt
- Bajo overhead

### Formatos de Datos

#### Estructura de Marcas JSON

```json
{
  "entrevista_id": "2025-10-05_003",
  "archivo_video": "data/videos_originales/entrevista_2025-10-05_003.mp4",
  "marcas": [
    {
      "entrevista_id": "2025-10-05_003",
      "pregunta_id": 1,
      "inicio": 5.2,
      "fin": 12.8,
      "nota": "Respuesta sobre semÃ¡foro"
    }
  ]
}
```

#### Estructura de Resultados de AnÃ¡lisis JSON

```json
{
  "fragmento": {
    "entrevista_id": "2025-10-05_003",
    "pregunta_id": 1,
    "ruta": "data/fragmentos/fragmento_2025-10-05_003_001.mp4"
  },
  "analisis": {
    "modelo": "cp_best_finetuned.h5",
    "fecha": "2025-10-05T14:30:00",
    "frames_analizados": 150,
    "resumen_emociones": {
      "dominant_emotion": "happy",
      "confidence": 0.85,
      "avg_intensities": {
        "angry": 0.05,
        "contempt": 0.02,
        "disgust": 0.01,
        "fear": 0.03,
        "happy": 0.85,
        "sad": 0.02,
        "surprise": 0.02
      }
    },
    "resultados_detallados": [
      {
        "frame": 1,
        "angry": 0.04,
        "happy": 0.90,
        ...
      }
    ]
  }
}
```

### Manejo de Errores y Logging

#### Estrategia de Logging

```python
# ConfiguraciÃ³n centralizada en main.py
logging.basicConfig(
    level=logging.DEBUG if debug else logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
```

#### Niveles de Logging Utilizados

- **DEBUG:** InformaciÃ³n detallada para diagnÃ³stico (carga de modelos, procesamiento de frames)
- **INFO:** Eventos importantes (inicio/fin de grabaciÃ³n, anÃ¡lisis completado)
- **WARNING:** Situaciones anÃ³malas pero recuperables (rostro no detectado, modelo no encontrado)
- **ERROR:** Errores que impiden funcionalidad (FFmpeg fallido, modelo corrupto)

#### Manejo de Excepciones

```python
try:
    analizador = Analisis(modelo_path)
except FileNotFoundError:
    logger.error(f"Modelo no encontrado: {modelo_path}")
    raise
except RuntimeError as e:
    logger.error(f"Error al cargar modelo: {e}")
    # Mostrar mensaje amigable al usuario
    QMessageBox.critical(self, "Error", "No se pudo cargar el modelo")
```

### Optimizaciones y Mejoras de Rendimiento

#### 1. Procesamiento AsÃ­ncrono

**Problema:** AnÃ¡lisis de videos largos congela la UI.

**SoluciÃ³n:** `QThread` para procesamiento en background.

```python
class AnalysisThread(QThread):
    progress_updated = Signal(int)
    log_message = Signal(str)
    finished_with_success = Signal(int, int)
    
    def run(self):
        # Procesamiento pesado aquÃ­
        # Emite seÃ±ales para actualizar UI
        for idx, fragmento in enumerate(self.fragmentos):
            # Analizar...
            self.progress_updated.emit(int((idx+1)/total*100))
```

#### 2. Carga Perezosa (Lazy Loading)

**Ejemplo en `Analisis`:**
```python
@property
def face_detection(self):
    if self._face_detection is None:
        # Solo inicializa cuando se necesita
        self._face_detection = mp.solutions.face_detection.FaceDetection(...)
    return self._face_detection
```

#### 3. Skip Frames

**OptimizaciÃ³n:** Procesar cada N frames en lugar de todos.

```python
if frame_count % skip_frames != 0:
    continue  # Saltar frame
```

**Trade-off:** Menor precisiÃ³n temporal vs. mayor velocidad.

### Testing y ValidaciÃ³n

#### Estrategia de Testing (Recomendada)

```python
# tests/test_entrevista.py
def test_marcar_inicio_pregunta():
    entrevista = Entrevista()
    entrevista.iniciar()
    pregunta_id = entrevista.marcar_inicio_pregunta()
    assert pregunta_id == 1
    assert len(entrevista.marcas.marcas) == 1

# tests/test_fragmento.py
def test_generar_fragmento():
    marca = Marca(entrevista_id="test", pregunta_id=1, inicio=0, fin=10)
    fragmento = Fragmento(marca, Path("test_fragmentos"))
    # Mock FFmpeg
    fragmento.generar_fragmento(Path("test_video.mp4"))
    assert fragmento.generado == True
```

### Seguridad y Consideraciones

#### ValidaciÃ³n de Entrada

- ValidaciÃ³n de rutas de archivos (evitar path traversal)
- ValidaciÃ³n de timestamps (no negativos, fin > inicio)
- ValidaciÃ³n de IDs (formato esperado)

#### Manejo de Recursos

- Cierre explÃ­cito de `VideoCapture` y `VideoWriter`
- Limpieza de procesos FFmpeg (SIGINT antes de kill)
- LiberaciÃ³n de recursos MediaPipe en destructor

---

## ğŸ“Š Resumen Ejecutivo

**AGRIOT UX** es una plataforma de investigaciÃ³n UX especializada en contextos agrÃ­colas que combina:

- **Arquitectura sÃ³lida** basada en principios SOLID y POO
- **Stack tecnolÃ³gico moderno** (PySide6, TensorFlow, MediaPipe, FFmpeg)
- **Procesamiento asÃ­ncrono** para anÃ¡lisis de video sin bloquear UI
- **Extensibilidad** mediante interfaces abstractas y patrones de diseÃ±o
- **Multiplataforma** con abstracciones para captura de video/audio

El sistema demuestra un **cumplimiento de SOLID del 84%** (8.4/10) con Ã¡reas de mejora identificadas en segregaciÃ³n de interfaces y extracciÃ³n de servicios adicionales.

**CalificaciÃ³n TÃ©cnica General: 8.5/10**

---

*DocumentaciÃ³n generada para proyecto_ux - AGRIOT UX Platform*
*Fecha: 2025*

